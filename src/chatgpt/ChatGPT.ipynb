{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A peek into ChatGPT\n",
        "* Evolution of ChatGPT : From neural network to large language model\n",
        "* Leveraging ChatGPT : From fine-tuning to zero-shot learning\n",
        "\n",
        "ChatGPT is a specialized language model for chat conversation. There is GPT in ChatGPT which stands for Generative Pretrained Transformer. We will try to demystify ChatGPT and build intuition toward fundamental concepts underpinning this technology.\n"
      ],
      "metadata": {
        "id": "lELlCyf4P6-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT sample\n",
        " \n",
        " ![ChatGPT-DogPrompt](https://drive.google.com/uc?export=view&id=1sJbH_2M9JDkfALNJJwCaFqTxqDV1qb-m)\n",
        "<br/>\n",
        "\n",
        "Visit [LearnGPT.com](https://www.learngpt.com/) to review other samples and learn more about ChatGPT.\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "SWV1ozJeBO5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function metaphor\n",
        "\n",
        "Both GPT and ChatGPT can be thought of as a function that transforms input string into output string. \n",
        "\n",
        "<br/>\n",
        "\n",
        " ![Function](https://drive.google.com/uc?export=view&id=1JMsfaMS1cO-5Gw0Wlm2o5w16dt4Nd82U)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mvkluLryCsj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing Functions with AI\n",
        "\n",
        "Function is a very general concept and as developers we have been writing functions for a long while. Functions map input(x) to certain output(y). Instead of writing functions in traditional way, deep learning allows us to build arbitrary functions with the help of AI. We now have a general machinery that given enough data(x) and supervision(y) can produce a the mapping function(aka model, f). This model f can then transform unseen input(x) to output(y). "
      ],
      "metadata": {
        "id": "_u9fruQrCpSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function Anatomy\n",
        "Our function consists of layers of neurons that receive input and transform it for next layer. Transformations depend on the strength of interconnections(aka weights) between neurons. \n",
        "* Training : We progressively tweak the weights such that quality of transformation improves.\n",
        "* Inference : We keep the weights frozen. We pass new input to the pre-trained model.\n",
        "\n",
        "<br/>\n",
        "\n",
        "![NeuralNet](https://drive.google.com/uc?export=view&id=1Tiwy1JJdP3Mu6URLVn1xjL0r8JQ0hbcv)\n",
        "\n"
      ],
      "metadata": {
        "id": "aHSeToLqZ1Va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing Functions with AI ( No training)"
      ],
      "metadata": {
        "id": "sdPA1_U7Tso8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reusing functions as-is\n",
        "\n",
        "We have a model hub that hosts pre-trained models(aka functions) for various tasks. Given a task, the library downloads the right function and use it to transform input to output. This works well if someone has already built functions that we need and they are available in the hub. \n",
        "\n",
        "Let us explore few tasks using HuggingFace libary.\n",
        "* Sentiment Analysis\n",
        "* Text Summarization\n",
        "* Text Generation\n",
        "\n"
      ],
      "metadata": {
        "id": "tNjjRxnjPyeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install hugging-face libraries\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "pLcl_DKeFsmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b373ac21-8c85-4076-a941-dc1fdab440c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.9.0)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load libraries\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "eW6vhYxfK84x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment Analysis\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "input_text = \"Its a sure-footed performance from an actor that never fails to hold the attention of its audience\"\n",
        "\n",
        "output_text = classifier(input_text)\n",
        "\n",
        "output_text"
      ],
      "metadata": {
        "id": "QUSzTNhMHBkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15714c89-1ccc-4566-bc23-7f6074c1571a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998669624328613}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Summarization\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "input_text = \"\"\"The James Webb Space Telescope has for the first time peered inside a planet-forming disk of dust surrounding a nearby star, \n",
        "                a development promising to supercharge the search for exoplanets. \n",
        "                Using the James Webb Space Telescope's near-infrared camera (NIRCam), a team of astronomers led by Kellen Lawson, \n",
        "                a postdoctoral program fellow at NASA Goddard Space Flight Center, observed the surroundings of a red dwarf star known as AU Microscopii or AU Mic. \n",
        "                Red dwarfs are rather unassuming stars that make up the largest population of stars in our galaxy, the Milky Way. \n",
        "                Most of the time, red dwarf stars are too faint to see in the visible light, which is why observing them in the heat-carrying infrared wavelengths, \n",
        "                those that Webb specializes in, is useful. Astronomers have known that AU Mic is surrounded by a planet-forming disk of gas and dust and have previously \n",
        "                discovered two exoplanets orbiting the star thanks to the star's periodic dimming caused by the planets' crossing in front of the star, \n",
        "                which was detected by NASA's exoplanet hunter TESS.\n",
        "               \"\"\"\n",
        "\n",
        "output_text = summarizer(input_text)\n",
        "\n",
        "output_text"
      ],
      "metadata": {
        "id": "Vp1jPtQ1IBmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bc6272-3e15-440f-f70a-68ffeed730b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': ' The James Webb Space Telescope has for the first time peered inside a planet-forming disk of dust surrounding a nearby star . Red dwarf stars are too faint to see in the visible light, which is why observing them in the heat-carrying infrared wavelengths is useful . Astronomers have previously discovered two exoplanets orbiting the star .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Generation\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "input_text = \"The greenhouse effect is essential to life on Earth, but human-made emissions are\"\n",
        "\n",
        "output_text = generator(input_text)\n",
        "\n",
        "output_text"
      ],
      "metadata": {
        "id": "ln4ZRm0gJgvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2f9d32-0a02-49dd-9f41-84256f379e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The greenhouse effect is essential to life on Earth, but human-made emissions are harming human life as much as possible.\\n\\nEven more harmful is the use of fossil fuels, which, in turn, are harming human life as well as nature,'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Models\n",
        "We can view these sample tasks as mapping from input(x) to output(y). In these cases, we relied on pre-existing models that were downloaded from the model hub.Since we did not do any training ourselves, all of these are examples of pre-trained models.\n",
        "\n",
        "|Task|Pre-trained model(f)|x|y|\n",
        "|---|---|---|--- |\n",
        "| Sentiment Analysis| distilbert-base-uncased-finetuned-sst-2-english  | text  |  sentiment(+ve, -ve) |\n",
        "| Summarizer| sshleifer/distilbart-cnn-12-6  | text  |  summary |\n",
        "| Generator| gpt2  | text  |  continuation text |\n",
        "\n",
        "<br/>\n",
        "\n",
        "Typically we do not find models that we need in the hub. The models available may have been trained on different task and fare poorly for our application. One way would be to build a new model from scratch. This process is expensive and data-hungry. Do we have relatively cheaper path to achieve good performance? Can we tweak original model for a different but related task?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "omDa1B8wOALj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusing functions with AI ( Partial training)\n",
        "\n",
        "All of us are familiar with tweaking the code base to support new scenario. We call it _refactoring_. Software engineers build layered architecture that enables code reuse through modular desgin. In a typical refactoring, most of the changes in the code happen at the top layer where earlier layers remain unchanged. We can appeal to this intuition when we do function reuse via **Transfer Learning**."
      ],
      "metadata": {
        "id": "2geVLOrtQu2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning - Concept\n",
        "A model is stacked layer of neurons. The earlier layers are grouped together as body. The latter layers are called head. We need to take the body of pre-trained model & attach a new head for our specific task. There are various approaches for transfer learning.\n",
        "* Feature extraction : Pretrained body is frozen, only head specific task is trained. This is less costly in terms of compute.\n",
        "* Fine tuning : Both the body and head is retrained though we retrain head parameters more. This is costlier but may achieve better task performance.\n",
        "\n",
        "![Transfer Learning](https://drive.google.com/uc?export=view&id=1XTfY6_piQdzEm0nmhec6uKZMMTitIVOf)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m9Qc9xkNScmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning - Motivation \n",
        "\n",
        "A natural question arises on why transfer learning works. Why does earlier weights of the model do not need to be altered for a new inference task? Let us look at a vision task to build some intuition. \n",
        "\n",
        "Zeiler and Fergus looked at various layers and found a way to visualize weights and activations to understand what the model is learning.\n",
        "\n",
        "We now look at a trained neural network on image classification task. Instead of looking at weights as a collection of numbers, we visualize these by looking at feature activation and corresponding images.\n",
        "\n",
        "\n",
        "* Layer 1-2\n",
        "![Layer1-2](https://drive.google.com/uc?export=view&id=15PQuA7c0uqcPb_CmpcWpT38gT8QxRbNl)\n",
        "\n",
        "\n",
        "* Layer 3\n",
        "![Layer3](https://drive.google.com/uc?export=view&id=1wDWyvPs5aHOdcxJDUw-dzsPEQhJSFQGU)\n",
        "\n",
        "\n",
        "* Layer 4-5\n",
        "![Layer4-5](https://drive.google.com/uc?export=view&id=1PWDZoUEBaHCkvq7vTCxw1Z06dqzqFxBq)\n",
        "\n",
        "\n",
        "<br>\n",
        "We realize that hidden layers progressively capture complex features as we move from input to output. This also means that earlier layers can be useful in many different tasks. For e.g. a model trained on image classification can be re-used for digit(1-9) recoginition. "
      ],
      "metadata": {
        "id": "yHOKAtDPSu9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT\n",
        "\n",
        "GPT is a large language model. \n",
        "\n",
        "* Large : GPT is large as it has large number of parameters. GPT-3 has 175 billion parameters. Large models have shown to be adept at doing tasks other than they were trained on.\n",
        "\n",
        "* Language model : GPT is a language model which is trained on large amount of text from the internet. The training task involves predicting next token given starting context. Once training is complete, GPT is able to spit out meaningful continuations from the starting text.\n",
        "\n"
      ],
      "metadata": {
        "id": "-mTMJvdSc2YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer family\n",
        "\n",
        "GPT belongs to a family of language models that were inspired by Transformer architecture. Even before the advent of Transformer, Encoder-Decoder models were common for translation task. The job of encoder was to encode input sequence such that it can be later used by decoder to generate sequence in another language. In the original paper [Attention is all you need](https://arxiv.org/abs/1706.03762), an encoder-decoder model was used for a translation task. Encoders and Decoders can also be seen as functions. If we consider english to french translation, we can break it down as follows.\n",
        "*  state = encoder(english-sentence)\n",
        "*  french-sentence = decoder(state)\n",
        "\n",
        "<br/>\n",
        "\n",
        "Overtime it was realized that we can build Tranformer models by just using one of the blocks(Encoder or Decoder). Now we have 3 different branches of Transformer based language models.\n",
        "\n",
        "* Encoder only : BERT\n",
        "* Decoder only : GPT\n",
        "* Encoder-Decoder : T5, BART\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "GPT is trained on next word prediciton task. Once trained, the model is able to complete a sentence given starting prompt. In essence, the model learns to mimic the internet.\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Transformer Family](https://drive.google.com/uc?export=view&id=1AwJqpLcxAwBETTV8yZYzIR2FLBTnTKfR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TUxwX93tUkMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention : Motivation\n",
        "Transformer was a very succesful architecture. One of the key ingredients was its use of attention. Attention is a general concept where decoder can pay attention to certain specific words of the encoder at the time of decoding. There is a related concept of self-attention where meaning of a word is derived from its neighboring context. Our language is replete with cases where context alters the meaning of a sentence and understanding this nuance is important to succesfully complete NLP tasks. \n",
        "\n",
        "![Self-Attention](https://drive.google.com/uc?export=view&id=1c6Bq6L762_uPkgFf94HtufyHfQMrB4GH)\n",
        "\n",
        "\n",
        "In the example, we can observe that the token 'flies' pays attention to different tokens in first and second sentence."
      ],
      "metadata": {
        "id": "Tv3wlVvbY9Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reusing function via Few-shot Learning\n",
        "Large language model exhibit few shot learning capabilities. Essentially we get good model performance without retraining the initial model on the desired task. The inference task is reformulated to bring it closer to training task. GPT can therefore be used to myriad new tasks without expensive retraining.\n",
        "\n",
        "![Few-Shot](https://drive.google.com/uc?export=view&id=1c4EnbgGit9hUzN7GWmObjjUMCudH0GxJ)\n",
        "\n"
      ],
      "metadata": {
        "id": "zoOiUmBWlZSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ways to build new functions\n",
        "\n",
        "We need new capabilities for our application. We have the following possibilities to explore.\n",
        "\n",
        "|Function metaphor|AI task| Considerations |\n",
        "|---|---| ---|\n",
        "| Write _new_ function| Training from scratch | Costly in compute and data  | \n",
        "| _Refactor_ existing function| Fine-tuning existing model| Moderate requirements on new data and compute  | \n",
        "| _Pass custom function_ to pre-existing function| Few shot learning, inference time conditioning | Cheapest, require large models, no training  |\n",
        "\n",
        "<br/>\n",
        "\n",
        "GPT and ChatGPT are large models that are expensive to train. Hence there is  lot of focus on exploiting few-shot learning on frozen language models. This opens the door for **Prompt Engineering**, a way to best use the large language models by building prompts optimized for the task at hand. \n",
        "\n"
      ],
      "metadata": {
        "id": "tYBKjq_4qQBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat GPT\n",
        "Chat GPT is language model specialized for chat conversation. It listens to our instructions and provides meaningful responses. Our initial GPT language model is fine-tuned to achieve this. Fine tuning is achieved in following ways.\n",
        "\n",
        "* Supervised fine tuning\n",
        "* RL based fine tuning : RLHF or Reinforcement Learning From Human Feedback"
      ],
      "metadata": {
        "id": "mGPG9luVfYrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RL setting\n",
        "\n",
        "![Reinforcement Learning](https://drive.google.com/uc?export=view&id=1eOctLUCRatLxnYCZUXPH8MpZWmOHUKFY)\n",
        "\n",
        "\n",
        "In reinforcement learning, we have an agent interacting with the environment.\n",
        "\n",
        "* Action : Agent takes an action against the environment.\n",
        "* Reward : Environment occasionally gives reward to agent.\n",
        "* State  : Agent receives a state from environment. Agent decides an action based on the state.\n",
        "* Policy : Policy maps a state to one of the actions. \n",
        "* Learning : Agent learns an optimal policy to maximize reward. An optimal policy gives us best action for a given state. \n",
        "\n",
        "For e.g. in a chess game, we have following notions.\n",
        "* Action : Chess move\n",
        "* Reward : Win or Loss\n",
        "* State  : State of the chess board\n",
        "* Policy : Given a state of chess board, take appropriate action.\n",
        "* Learning : Learn best policy in each state so as to win the chess game.\n",
        "\n",
        "\n",
        "We are given an initial learning model. We would like to have a reward signal that that grounds us in human values and guides the model to produce good quality response that are honest, helpfule and harmless.\n",
        "\n",
        "Once the model is fine-tuned, we hope to get a good quality chat engine that acts as an AI assistant.\n"
      ],
      "metadata": {
        "id": "Z1wrw6E5hJ8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Fine Tuning\n",
        "\n",
        "In supervised fine tuning, we take combination of prompts and responses. We also add high quality human responses for a given prompt to the mix. This training data can then be used to fine-tune the initial language model using Transfer Learning.\n",
        "\n",
        "![Supervised Fine Tuning](https://drive.google.com/uc?export=view&id=1ySIZBn4BSxTOcQhoGAZ9jU3OFuDhgMo2)\n",
        "\n"
      ],
      "metadata": {
        "id": "luE90QlmmNlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RLHF : Reinforcement Learning From Human Feedback\n",
        "\n",
        "The goal of RLHF is to provide human grounding to the model results beyond the loss function used to build the inital language model. We will be using RL for this step. The initial model finds itself in some state based on initial prompt. We would like the model to learn a policy(i.e. pick best action which means a meaningful continuation of conversation) such that we have responses that are highly valued by humans. \n",
        "\n",
        "How do we go about shaping the reward function with human inputs? \n",
        "\n",
        "We will need a _reward model_ to score model response. Once we have a good reward model, we can use it to fine tune the _initial language model_.\n",
        "\n"
      ],
      "metadata": {
        "id": "4IRIghsdnx6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reward Model\n",
        "We sample multiple responses to a given prompt from the language model. These responses are ranked by humans. A high ranked response corresponds to more reward. This data is used to build a **Reward Model** that transforms given prompt-response into a scalar reward.\n",
        "\n",
        "![Reward Model](https://drive.google.com/uc?export=view&id=1_4S4JwixPCyYKM_HfPSZml5fmsIibB_c)\n",
        "\n"
      ],
      "metadata": {
        "id": "kMQLS2CTYRm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RL Fine tuning\n",
        "We begin with 2 identical language models with the goal of tuning one of these using Reinforcement Learning. The fine tuning reward has 2 components.\n",
        "\n",
        "* Human Reward : The earlier reward model is used to get the reward for the response from fine-tuned model. \n",
        "\n",
        "* Prediction Shift Penalty : Same prompt is passed to both the models and their divergence is measured. This constraints the fine-tuned model from diverging too much from the initial language model. This is similar to how we freeze initial stacks of neural network while doing Transfer Learning in supervised setting.\n",
        "\n",
        "Combined reward is used to guide the update to the language model. Once training is complete, we get our **ChatGPT**.\n",
        "\n",
        "\n",
        "![RLHF](https://drive.google.com/uc?export=view&id=1_kN-xD5C_IO_OjKS8W3BU7SJmH2kaZG2)"
      ],
      "metadata": {
        "id": "Dj0auX0fZVPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications\n",
        "\n",
        "Large language models(LLMs) like ChatGPT gives us language generation abilities coupled with a vast trove of knowledge base. They are also capable of in-context learning where they can learn a new task by following few examples. This gives us a wide design space to imagine and build new applications. LLM would be a great asset in your dev toolbox. So, go forth and **prompt** !!\n",
        "\n",
        "Here is a sample from new Bing. Bing is prompted to solve a riddle that appears to be gibberish. Bing is able to do a remarkable job by leveraging its LLM.\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Bing Riddle](https://drive.google.com/uc?export=view&id=1MxN05lXUkCo6p2KoPBQWw-tirpmcWl_6)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AAbD71FVPmXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "* [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "* [Chat GPT: Optimizing Language Model for dialogue](https://openai.com/blog/chatgpt/)\n",
        "* [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "* [InstructGPT : Training language models to follow instructions\n",
        "with human feedback](https://arxiv.org/pdf/2203.02155.pdf)\n",
        "* [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "* [Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
        "* [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)\n"
      ],
      "metadata": {
        "id": "ovtWs30ihNaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Credits\n",
        "* [Bing riddle](https://twitter.com/thomasrice_au/status/1627128539886780417?lang=en)\n",
        "* [Illustrating RLHF](https://huggingface.co/blog/rlhf)\n",
        "* [Language models are few shot learners](https://arxiv.org/pdf/2005.14165v4.pdf)\n",
        "* [Transformers Book](https://transformersbook.com/)\n",
        "* [Visualizing and Understanding CNN](https://arxiv.org/abs/1311.2901)\n",
        "\n"
      ],
      "metadata": {
        "id": "EmaKZ_oaJK5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions"
      ],
      "metadata": {
        "id": "iY6so7jhjfeB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKScadIDjieP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
